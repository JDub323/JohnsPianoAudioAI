batch_size: 16
epochs: 100
learning_rate: .001 
optimizer: adam # or SGD
scheduler: cosine
weight_decay: .0001 
grad_clip: 1.0 

use_gpu: True

checkpoint_dir: ./models/checkpoints/ 
logging_dir: ./outputs/

loss_function: 
  activations: BCElogits
  onsets: BCElogits 
  velocities: MSE # or L1 loss 
loss_weights:
  activations: 1.0
  onsets: 2.0 
  velocities: 0.1 # typically even smaller, normalize if needed

early_stopping: True
early_stop:
  patience: 10
  delta: 0.0
