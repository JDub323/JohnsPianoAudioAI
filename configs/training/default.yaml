batch_size: 16
epochs: 100
learning_rate: .001 
optimizer: adam
scheduler: cosine
weight_decay: .0001 
grad_clip: 1.0 

loss_function: 
  frames: bce 
  onsets: bce 
  velocities: mse # or L1 loss 
loss_weights:
  frames: 1.0
  onsets: 2.0 
  velocities: 0.1 # typically even smaller, normalize if needed

