name: MAESTRO 
row_limit: -1 # -1 means unbound, so it will go through all rows.
data_root: ./data/unprocessed/maestro-v3.0.0/
export_root: ./data/processed/
spreadsheet_name: maestro-v3.0.0.csv 
processed_csv_name: processed_data.csv
sample_rate: 16000 
num_channels: 1
midis_exist: True 

transform:
  input_representation: mel # or CQT, or non-negative matrix factorization of the input
  n_bins: 88 
  hop_length: 512 
  filter_scale: 1 # small means higher frequency resolution, big means higher time resolution
  window_size: 2048
  window_type: hann
  scale: True
  f_min: 27.5 # may want to account for ralisback curve by lowering the minimum frequency by arbitrary ~15 cents (should be 27.5)
  f_max: 4186 # these are frequency minimum and maximums btw. This is not used as of 8/6/25, since n_bins and f_min dictate this

normalize_audio: false # experamental function to make all piano audios roughly the same loudness
augment_audio_data: true
samples_silence: 200000 # Audiomentations are still present, so not "silent". APX. 5 seconds (AFAIK... TODO: CHECK THIS)
noise_root: /mnt/c/Users/jwhal/Projects/PythonProjects/JohnsPianoAudioAI/data/augmentations # use of abs path only for debugging
noise_paths:
  environmental: ./data/augmentations/environment_resampled/
  speech: ./data/augmentations/LibriSpeech/train-clean-100/
  RIR: ./data/augmentations/RIR_resampled/
  DIR: ./data/augmentations/DIR_full_resampled/
augmentation_probabilities: # THIS PIPELINE AND PROBABILITY DIST. WAS TAKEN FROM Mobile-AMT 
  pitch_shift: .5 
  speech: .5
  environmental: .5 # do this before room impulse response
  RIR: 1 # room impulse response convolution
  stationary: .5
  DIR: 1 # device impulse response convolution
  clamp: .05 # audio clipping

use_default_splits: True
segment_length: 5
sharding:
  samples_per_shard: 2000 # This makes around 20 shards for the whole dataset, with each shard being ~1/20 of the whole dataset
